2025-05-10 14:54:33,320 - INFO - Found API key in environment variables
2025-05-10 14:54:33,785 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:54:33,791 - INFO - Groq API key verified successfully!
2025-05-10 14:54:33,792 - INFO - Starting preload of models...
2025-05-10 14:54:33,792 - INFO - Initializing embeddings model...
2025-05-10 14:54:33,806 - WARNING -  * Debugger is active!
2025-05-10 14:54:33,815 - INFO -  * Debugger PIN: 865-307-807
2025-05-10 14:55:39,125 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 14:55:40,273 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 14:55:40,275 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 14:55:40,277 - INFO - JAX version 0.5.3 available.
2025-05-10 14:55:46,208 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 14:55:50,565 - INFO - Embeddings model initialized successfully
2025-05-10 14:55:50,565 - INFO - Creating new vectorstore for language: en...
2025-05-10 14:55:50,818 - INFO - Loading faiss with AVX2 support.
2025-05-10 14:55:51,282 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 14:55:51,297 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 14:55:51,302 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 14:55:51,302 - INFO - Creating new vectorstore for language: hi...
2025-05-10 14:55:51,491 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 14:55:51,491 - INFO - Model preloading completed!
2025-05-10 14:56:57,237 - INFO - 127.0.0.1 - - [10/May/2025 14:56:57] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:56:57,490 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:56:57,490 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 14:56:57,490 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:56:57,490 - INFO - Found API key in environment variables
2025-05-10 14:56:57,999 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:56:58,002 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:56:59,582 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:56:59,623 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:56:59,627 - INFO - 127.0.0.1 - - [10/May/2025 14:56:59] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:14,141 - INFO - 127.0.0.1 - - [10/May/2025 14:59:14] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:14,390 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:59:14,390 - INFO - [PROCESSING] Question: Apps not working on my TV, Language: en
2025-05-10 14:59:14,390 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:59:14,390 - INFO - Found API key in environment variables
2025-05-10 14:59:14,821 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:59:14,821 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:59:16,057 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:59:16,063 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:59:16,063 - INFO - 127.0.0.1 - - [10/May/2025 14:59:16] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:31,918 - INFO - 127.0.0.1 - - [10/May/2025 14:59:31] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:32,223 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:59:32,223 - INFO - [PROCESSING] Question: Wi-Fi issues on old Samsung TV, Language: en
2025-05-10 14:59:32,223 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:59:32,223 - INFO - Found API key in environment variables
2025-05-10 14:59:32,633 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:59:32,633 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:59:33,785 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:59:33,785 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:59:33,785 - INFO - 127.0.0.1 - - [10/May/2025 14:59:33] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:29,142 - INFO - 127.0.0.1 - - [10/May/2025 15:00:29] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:29,451 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:00:29,451 - INFO - [PROCESSING] Question: m3164, Language: en
2025-05-10 15:00:29,451 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:00:29,451 - INFO - Found API key in environment variables
2025-05-10 15:00:29,922 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:00:29,924 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:00:31,214 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    from langchain_core.prompts import PromptTemplate
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 15:00:31,222 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    # Get QA Chain with the pre-trained vectorstore
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    # Create a robust fallback
                             ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 15:00:31,222 - INFO - 127.0.0.1 - - [10/May/2025 15:00:31] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:50,305 - INFO - 127.0.0.1 - - [10/May/2025 15:00:50] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:50,570 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:00:50,570 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:00:50,570 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:00:50,570 - INFO - Found API key in environment variables
2025-05-10 15:00:51,089 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:00:51,089 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:00:52,231 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    from langchain_core.prompts import PromptTemplate
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 15:00:52,231 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    # Get QA Chain with the pre-trained vectorstore
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    # Create a robust fallback
                             ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 15:00:52,235 - INFO - 127.0.0.1 - - [10/May/2025 15:00:52] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:05,967 - INFO - Found API key in environment variables
2025-05-10 15:01:06,450 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:06,464 - INFO - Groq API key verified successfully!
2025-05-10 15:01:06,465 - INFO - Starting preload of models...
2025-05-10 15:01:06,465 - INFO - Initializing embeddings model...
2025-05-10 15:01:06,490 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:01:06,491 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:01:10,192 - INFO - 127.0.0.1 - - [10/May/2025 15:01:10] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:10,442 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:10,442 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:01:10,442 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:10,442 - INFO - Found API key in environment variables
2025-05-10 15:01:10,866 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:10,870 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:10,870 - INFO - Initializing embeddings model...
2025-05-10 15:01:26,907 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:01:27,739 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:01:27,739 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:01:27,750 - INFO - JAX version 0.5.3 available.
2025-05-10 15:01:29,261 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:01:29,270 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:01:32,838 - INFO - Embeddings model initialized successfully
2025-05-10 15:01:32,838 - INFO - Loading existing index from faiss_en_index...
2025-05-10 15:01:32,844 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:01:32,844 - INFO - Creating new vectorstore for language: en...
2025-05-10 15:01:32,949 - INFO - Loading faiss with AVX2 support.
2025-05-10 15:01:32,990 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 15:01:33,000 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 15:01:33,004 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 15:01:33,296 - INFO - Embeddings model initialized successfully
2025-05-10 15:01:33,296 - INFO - Loading existing index from faiss_hi_index...
2025-05-10 15:01:33,296 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:01:33,300 - INFO - Creating new vectorstore for language: hi...
2025-05-10 15:01:33,483 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 15:01:33,484 - INFO - Model preloading completed!
2025-05-10 15:01:34,508 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:35,135 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:35,151 - INFO - Request processed in 24.71 seconds
2025-05-10 15:01:35,151 - INFO - 127.0.0.1 - - [10/May/2025 15:01:35] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:46,663 - INFO - 127.0.0.1 - - [10/May/2025 15:01:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:46,972 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:46,972 - INFO - [PROCESSING] Question: my tv is not working, Language: en
2025-05-10 15:01:46,972 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:46,972 - INFO - Found API key in environment variables
2025-05-10 15:01:47,421 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:47,427 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:48,531 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:49,164 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:49,164 - INFO - Request processed in 2.19 seconds
2025-05-10 15:01:49,164 - INFO - 127.0.0.1 - - [10/May/2025 15:01:49] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:57,570 - INFO - 127.0.0.1 - - [10/May/2025 15:01:57] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:57,882 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:57,882 - INFO - [PROCESSING] Question: it is in safe mode, Language: en
2025-05-10 15:01:57,882 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:57,882 - INFO - Found API key in environment variables
2025-05-10 15:01:58,261 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:58,261 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:59,372 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:59,706 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:59,712 - INFO - Request processed in 1.83 seconds
2025-05-10 15:01:59,712 - INFO - 127.0.0.1 - - [10/May/2025 15:01:59] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:06:59,800 - INFO - Found API key in environment variables
2025-05-10 15:07:00,327 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:07:00,329 - INFO - Groq API key verified successfully!
2025-05-10 15:07:00,330 - INFO - Starting preload of models...
2025-05-10 15:07:00,330 - INFO - Initializing embeddings model...
2025-05-10 15:07:00,352 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:07:00,352 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:07:06,935 - INFO - 127.0.0.1 - - [10/May/2025 15:07:06] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:07:07,320 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:07:08,828 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:07:08,828 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:07:08,829 - INFO - Found API key in environment variables
2025-05-10 15:07:09,299 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:07:09,304 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:07:09,304 - INFO - Initializing embeddings model...
2025-05-10 15:07:22,467 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:07:23,018 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:07:23,021 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:07:23,024 - INFO - JAX version 0.5.3 available.
2025-05-10 15:07:24,570 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:24,572 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:27,831 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:27,873 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:27,873 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-10 15:07:27,882 - INFO - Initializing embeddings model...
2025-05-10 15:07:27,884 - ERROR - QA Chain is None - Using direct fallback
2025-05-10 15:07:27,884 - INFO - 127.0.0.1 - - [10/May/2025 15:07:27] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:07:27,890 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:30,482 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:30,497 - INFO - Initializing embeddings model...
2025-05-10 15:07:30,500 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:33,237 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:33,242 - INFO - Model preloading completed!
2025-05-10 15:09:04,935 - INFO - Found API key in environment variables
2025-05-10 15:09:05,446 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:09:05,447 - INFO - Groq API key verified successfully!
2025-05-10 15:09:05,447 - INFO - Starting preload of models...
2025-05-10 15:09:05,470 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:09:05,470 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:09:25,571 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:09:26,082 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:09:26,082 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:09:26,095 - INFO - JAX version 0.5.3 available.
2025-05-10 15:09:27,914 - INFO - Initializing embeddings model...
2025-05-10 15:09:29,181 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:29,181 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:34,886 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-10 15:09:34,901 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:34,901 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:37,709 - INFO - Fallback embeddings model initialized successfully
2025-05-10 15:09:37,709 - INFO - Loading existing index from faiss_en_index...
2025-05-10 15:09:37,709 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:09:37,709 - INFO - Creating new vectorstore for language: en...
2025-05-10 15:09:37,709 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:37,709 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:40,464 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:40,464 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-10 15:09:43,874 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-10 15:11:46,494 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-10 15:11:54,608 - INFO - Loading faiss with AVX2 support.
2025-05-10 15:11:54,970 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 15:11:54,987 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 15:11:54,992 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 15:11:54,993 - INFO - Loading existing index from faiss_hi_index...
2025-05-10 15:11:54,993 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:11:54,993 - INFO - Creating new vectorstore for language: hi...
2025-05-10 15:11:54,997 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:11:55,000 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:11:58,081 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:11:58,081 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-10 15:12:02,650 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 15:12:02,650 - INFO - Model preloading completed!
2025-05-10 15:35:23,729 - INFO - 127.0.0.1 - - [10/May/2025 15:35:23] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:23,990 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:35:24,657 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:35:24,657 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:35:24,657 - INFO - Found API key in environment variables
2025-05-10 15:35:25,123 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:35:25,123 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:35:26,490 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:35:27,120 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:35:27,132 - INFO - Request processed in 3.14 seconds
2025-05-10 15:35:27,132 - INFO - 127.0.0.1 - - [10/May/2025 15:35:27] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:39,750 - INFO - 127.0.0.1 - - [10/May/2025 15:35:39] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:40,061 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:35:40,066 - INFO - [PROCESSING] Question: lg tv is not working, Language: en
2025-05-10 15:35:40,066 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:35:40,066 - INFO - Found API key in environment variables
2025-05-10 15:35:41,000 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:35:41,000 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:35:42,211 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:35:42,815 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:35:42,820 - INFO - Request processed in 2.76 seconds
2025-05-10 15:35:42,822 - INFO - 127.0.0.1 - - [10/May/2025 15:35:42] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:24,748 - INFO - 127.0.0.1 - - [10/May/2025 15:36:24] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:25,051 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:36:25,078 - INFO - [PROCESSING] Question: TV starting in Safe Mode, Language: en
2025-05-10 15:36:25,078 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:36:25,079 - INFO - Found API key in environment variables
2025-05-10 15:36:25,448 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:36:25,449 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:36:26,650 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:36:26,988 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:36:26,995 - INFO - Request processed in 1.94 seconds
2025-05-10 15:36:26,995 - INFO - 127.0.0.1 - - [10/May/2025 15:36:26] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:36,776 - INFO - 127.0.0.1 - - [10/May/2025 15:36:36] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:37,082 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:36:37,093 - INFO - [PROCESSING] Question: Wi-Fi issues on old Samsung TV, Language: en
2025-05-10 15:36:37,093 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:36:37,093 - INFO - Found API key in environment variables
2025-05-10 15:36:37,470 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:36:37,474 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:36:38,689 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:36:39,134 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:36:39,134 - INFO - Request processed in 2.05 seconds
2025-05-10 15:36:39,134 - INFO - 127.0.0.1 - - [10/May/2025 15:36:39] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:35:01,114 - INFO - Found API key in environment variables
2025-05-12 18:35:01,692 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:35:01,693 - INFO - Groq API key verified successfully!
2025-05-12 18:35:01,695 - INFO - Starting preload of models...
2025-05-12 18:35:01,766 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:35:01,766 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:35:46,429 - INFO - 127.0.0.1 - - [12/May/2025 18:35:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:35:46,701 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:35:48,976 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:35:48,976 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:35:48,976 - INFO - Found API key in environment variables
2025-05-12 18:35:49,579 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:35:49,581 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:36:15,746 - INFO - Found API key in environment variables
2025-05-12 18:36:16,193 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:36:16,193 - INFO - Groq API key verified successfully!
2025-05-12 18:36:16,195 - INFO - Starting preload of models...
2025-05-12 18:36:16,227 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:36:16,227 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:36:23,691 - INFO - 127.0.0.1 - - [12/May/2025 18:36:23] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:36:23,909 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:36:24,758 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:36:24,758 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:36:24,758 - INFO - Found API key in environment variables
2025-05-12 18:36:25,338 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:36:25,350 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:37:18,052 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-12 18:37:19,210 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-12 18:37:19,213 - INFO - TensorFlow version 2.19.0 available.
2025-05-12 18:37:19,242 - INFO - JAX version 0.5.3 available.
2025-05-12 18:37:26,839 - INFO - Initializing embeddings model...
2025-05-12 18:37:26,839 - INFO - Initializing embeddings model...
2025-05-12 18:37:28,214 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:28,214 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:28,217 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:28,217 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:33,852 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:33,913 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:33,913 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:33,948 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:33,954 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:33,954 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:36,627 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:36,683 - INFO - Initializing embeddings model...
2025-05-12 18:37:36,689 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:36,689 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:37,071 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:37,085 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:37:37,085 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:37:37,103 - INFO - 127.0.0.1 - - [12/May/2025 18:37:37] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:37:39,360 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:39,364 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:39,369 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:42,070 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:42,085 - INFO - Initializing embeddings model...
2025-05-12 18:37:42,093 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:42,093 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:44,780 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:44,787 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:44,787 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:49,036 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:49,042 - INFO - Model preloading completed!
2025-05-12 18:38:00,171 - INFO - 127.0.0.1 - - [12/May/2025 18:38:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:38:00,433 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:38:00,470 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:38:00,472 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:38:00,472 - INFO - Found API key in environment variables
2025-05-12 18:38:00,932 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:38:00,932 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:38:00,932 - INFO - Initializing embeddings model...
2025-05-12 18:38:00,941 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:38:00,941 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:38:04,189 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:38:04,199 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:38:04,199 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:38:06,967 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:38:06,984 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:38:06,984 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:38:06,986 - INFO - 127.0.0.1 - - [12/May/2025 18:38:06] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:45:48,749 - INFO - Found API key in environment variables
2025-05-12 18:45:49,157 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:45:49,157 - INFO - Groq API key verified successfully!
2025-05-12 18:45:49,160 - INFO - Starting preload of models...
2025-05-12 18:45:49,199 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:45:49,199 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:45:59,479 - INFO - 127.0.0.1 - - [12/May/2025 18:45:59] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:45:59,734 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:46:00,593 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:46:00,597 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:46:00,597 - INFO - Found API key in environment variables
2025-05-12 18:46:01,077 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:46:01,077 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:46:20,177 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-12 18:46:21,046 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-12 18:46:21,046 - INFO - TensorFlow version 2.19.0 available.
2025-05-12 18:46:21,046 - INFO - JAX version 0.5.3 available.
2025-05-12 18:46:23,482 - INFO - Initializing embeddings model...
2025-05-12 18:46:23,482 - INFO - Initializing embeddings model...
2025-05-12 18:46:24,732 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:24,735 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:24,735 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:24,735 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:28,844 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:28,844 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:28,867 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:28,870 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:28,870 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:28,870 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:31,421 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:31,436 - INFO - Initializing embeddings model...
2025-05-12 18:46:31,442 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:31,442 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:31,512 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:31,518 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:46:31,518 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:46:31,527 - INFO - 127.0.0.1 - - [12/May/2025 18:46:31] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:46:34,238 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:34,244 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:34,244 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:36,802 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:36,814 - INFO - Initializing embeddings model...
2025-05-12 18:46:36,820 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:36,820 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:39,457 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:39,464 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:39,464 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:42,199 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:42,219 - INFO - Model preloading completed!
2025-05-12 18:47:00,859 - INFO - 127.0.0.1 - - [12/May/2025 18:47:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:47:01,113 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:47:01,126 - INFO - [PROCESSING] Question: tv not working, Language: en
2025-05-12 18:47:01,126 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:47:01,126 - INFO - Found API key in environment variables
2025-05-12 18:47:01,637 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:47:01,637 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:47:01,637 - INFO - Initializing embeddings model...
2025-05-12 18:47:01,644 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:47:01,644 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:47:04,921 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:47:04,931 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:47:04,932 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:47:08,031 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:47:08,044 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:47:08,044 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:47:08,049 - INFO - 127.0.0.1 - - [12/May/2025 18:47:08] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:22:02,629 - INFO - Found API key in environment variables
2025-05-12 19:22:03,175 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:22:03,177 - INFO - Groq API key verified successfully!
2025-05-12 19:22:03,179 - INFO - Starting preload of models...
2025-05-12 19:22:03,241 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:22:03,241 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:22:25,537 - INFO - Initializing embeddings model...
2025-05-12 19:22:25,541 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:25,541 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:30,343 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:22:30,361 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:30,361 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:33,407 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:22:33,407 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:22:33,407 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:33,407 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:22:33,416 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:33,416 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:36,209 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:36,209 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:41,256 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,332 - INFO - 127.0.0.1 - - [12/May/2025 19:22:41] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:22:41,377 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,586 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:22:41,861 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,861 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:41,861 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:22:41,863 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:41,863 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:22:41,867 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:41,867 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:43,932 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:22:43,932 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:22:43,932 - INFO - Found API key in environment variables
2025-05-12 19:22:44,345 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:22:44,347 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:22:44,347 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:22:44,347 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:44,347 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:22:44,356 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:44,356 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:45,482 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:45,487 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:47,380 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:47,380 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:50,166 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,361 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,590 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,590 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:50,590 - INFO - Model preloading completed!
2025-05-12 19:22:53,791 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:53,896 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:54,106 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:54,106 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:54,106 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 19:22:54,109 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 19:22:54,110 - INFO - 127.0.0.1 - - [12/May/2025 19:22:54] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:23,692 - INFO - Found API key in environment variables
2025-05-12 19:25:24,195 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:25:24,201 - INFO - Groq API key verified successfully!
2025-05-12 19:25:24,203 - INFO - Starting preload of models...
2025-05-12 19:25:24,265 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:25:24,265 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:25:32,793 - INFO - 127.0.0.1 - - [12/May/2025 19:25:32] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:33,045 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:25:33,881 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:25:33,881 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:25:33,881 - INFO - Found API key in environment variables
2025-05-12 19:25:34,510 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:25:34,519 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:25:37,360 - INFO - Initializing embeddings model...
2025-05-12 19:25:37,360 - INFO - Initializing embeddings model...
2025-05-12 19:25:37,360 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:37,360 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:37,360 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:37,360 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:40,661 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:25:40,676 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:40,676 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:40,876 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:25:40,879 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:40,879 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:43,737 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:25:43,737 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:25:43,739 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:43,739 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:25:43,744 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:43,744 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:43,844 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:25:43,845 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:25:43,845 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:43,845 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:25:43,849 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:43,849 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:48,077 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:48,077 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:25:48,077 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:48,077 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:25:54,956 - INFO - Loading faiss with AVX2 support.
2025-05-12 19:25:55,090 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 397, in get_qa_chain
    vectorstore = get_vectorstore(language)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 206, in get_vectorstore
    return train_vectorstore(language)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 176, in train_vectorstore
    HuggingFaceEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:25:55,384 - INFO - Request processed in 22.34 seconds
2025-05-12 19:25:55,386 - INFO - 127.0.0.1 - - [12/May/2025 19:25:55] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:55,587 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-12 19:25:55,602 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-12 19:25:55,613 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-12 19:25:55,613 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:25:55,613 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:55,613 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:25:55,620 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:55,620 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:58,679 - ERROR - Error during model preloading: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 479, in preload_models
    get_vectorstore("hi")
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 206, in get_vectorstore
    return train_vectorstore(language)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 175, in train_vectorstore
    HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:22,265 - INFO - Found API key in environment variables
2025-05-12 19:39:22,743 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:39:22,743 - INFO - Groq API key verified successfully!
2025-05-12 19:39:22,745 - INFO - Starting preload of models...
2025-05-12 19:39:22,788 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:39:22,788 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:39:30,480 - INFO - 127.0.0.1 - - [12/May/2025 19:39:30] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:39:30,741 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:39:31,561 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:39:31,563 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:39:31,563 - INFO - Found API key in environment variables
2025-05-12 19:39:31,993 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:39:31,993 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:39:35,861 - INFO - Initializing embeddings model...
2025-05-12 19:39:35,861 - INFO - Initializing embeddings model...
2025-05-12 19:39:35,865 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:35,865 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:35,865 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:35,867 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:40,033 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:39:40,049 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:40,049 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:40,236 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:40,241 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:40,241 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:43,006 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:43,023 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 19:39:43,025 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 19:39:43,026 - INFO - 127.0.0.1 - - [12/May/2025 19:39:43] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:39:43,039 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:43,053 - INFO - Initializing embeddings model...
2025-05-12 19:39:43,064 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:43,066 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:46,675 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:46,678 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:46,680 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:49,311 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:49,325 - INFO - Initializing embeddings model...
2025-05-12 19:39:49,328 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:49,328 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:52,176 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:52,176 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:52,176 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:55,326 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:55,338 - INFO - Model preloading completed!
2025-05-12 19:41:16,686 - INFO - Found API key in environment variables
2025-05-12 19:41:17,216 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:41:17,216 - INFO - Groq API key verified successfully!
2025-05-12 19:41:17,216 - INFO - Starting preload of models...
2025-05-12 19:41:17,272 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:41:17,272 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:41:28,374 - INFO - Initializing embeddings model...
2025-05-12 19:41:28,376 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:28,376 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:31,778 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:41:31,794 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:31,797 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:34,350 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:41:34,352 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:41:34,354 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:41:34,354 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:41:34,357 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:34,357 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:37,265 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:37,265 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:41:41,423 - INFO - 127.0.0.1 - - [12/May/2025 19:41:41] "OPTIONS /api/complaints HTTP/1.1" 200 -
2025-05-12 19:41:41,462 - INFO - 127.0.0.1 - - [12/May/2025 19:41:41] "POST /api/complaints HTTP/1.1" 200 -
2025-05-12 19:41:42,202 - INFO - Loading faiss with AVX2 support.
2025-05-12 19:41:42,272 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-12 19:41:42,294 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-12 19:41:42,299 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-12 19:41:42,300 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:41:42,301 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:41:42,301 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:41:42,308 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:42,308 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:44,992 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:44,992 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:41:49,815 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-12 19:41:49,815 - INFO - Model preloading completed!
2025-05-12 19:42:00,689 - INFO - 127.0.0.1 - - [12/May/2025 19:42:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:00,945 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:02,450 - INFO - [PROCESSING] Question: tv not working in safe mode, Language: en
2025-05-12 19:42:02,450 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:02,450 - INFO - Found API key in environment variables
2025-05-12 19:42:02,963 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:02,963 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:04,834 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:05,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:05,350 - INFO - Request processed in 4.40 seconds
2025-05-12 19:42:05,364 - INFO - 127.0.0.1 - - [12/May/2025 19:42:05] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:18,118 - INFO - 127.0.0.1 - - [12/May/2025 19:42:18] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:18,432 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:18,437 - INFO - [PROCESSING] Question: lg tv not working, Language: en
2025-05-12 19:42:18,437 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:18,437 - INFO - Found API key in environment variables
2025-05-12 19:42:18,824 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:18,825 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:20,434 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:20,835 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:20,843 - INFO - Request processed in 2.41 seconds
2025-05-12 19:42:20,843 - INFO - 127.0.0.1 - - [12/May/2025 19:42:20] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:39,836 - INFO - 127.0.0.1 - - [12/May/2025 19:42:39] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:40,152 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:40,188 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:42:40,188 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:40,188 - INFO - Found API key in environment variables
2025-05-12 19:42:40,645 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:40,645 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:42,112 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:42,673 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:42,676 - INFO - Request processed in 2.52 seconds
2025-05-12 19:42:42,679 - INFO - 127.0.0.1 - - [12/May/2025 19:42:42] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:50,466 - INFO - 127.0.0.1 - - [12/May/2025 19:42:50] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:50,770 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:50,776 - INFO - [PROCESSING] Question: only samsung, Language: en
2025-05-12 19:42:50,776 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:50,776 - INFO - Found API key in environment variables
2025-05-12 19:42:51,149 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:51,151 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:52,716 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:52,992 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:52,994 - INFO - Request processed in 2.22 seconds
2025-05-12 19:42:52,994 - INFO - 127.0.0.1 - - [12/May/2025 19:42:52] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:43:46,795 - INFO - 127.0.0.1 - - [12/May/2025 19:43:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:43:47,108 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:43:47,130 - INFO - [PROCESSING] Question: nokia mobile not working, Language: en
2025-05-12 19:43:47,130 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:43:47,133 - INFO - Found API key in environment variables
2025-05-12 19:43:47,616 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:43:47,616 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:43:49,229 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:43:49,838 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:43:49,841 - INFO - Request processed in 2.73 seconds
2025-05-12 19:43:49,841 - INFO - 127.0.0.1 - - [12/May/2025 19:43:49] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:11,057 - INFO - 127.0.0.1 - - [12/May/2025 19:44:11] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:11,373 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:44:11,382 - INFO - [PROCESSING] Question: samsung mobile not working, Language: en
2025-05-12 19:44:11,382 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:44:11,382 - INFO - Found API key in environment variables
2025-05-12 19:44:11,790 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:44:11,793 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:44:13,185 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:44:13,725 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:44:13,728 - INFO - Request processed in 2.35 seconds
2025-05-12 19:44:13,728 - INFO - 127.0.0.1 - - [12/May/2025 19:44:13] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:29,283 - INFO - 127.0.0.1 - - [12/May/2025 19:44:29] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:29,601 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:44:29,607 - INFO - [PROCESSING] Question: lg mobile not working, Language: en
2025-05-12 19:44:29,613 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:44:29,613 - INFO - Found API key in environment variables
2025-05-12 19:44:30,108 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:44:30,108 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:44:31,697 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:44:32,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:44:32,465 - INFO - Request processed in 2.86 seconds
2025-05-12 19:44:32,468 - INFO - 127.0.0.1 - - [12/May/2025 19:44:32] "POST /api/faq HTTP/1.1" 200 -
