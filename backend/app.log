2025-05-10 14:54:33,320 - INFO - Found API key in environment variables
2025-05-10 14:54:33,785 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:54:33,791 - INFO - Groq API key verified successfully!
2025-05-10 14:54:33,792 - INFO - Starting preload of models...
2025-05-10 14:54:33,792 - INFO - Initializing embeddings model...
2025-05-10 14:54:33,806 - WARNING -  * Debugger is active!
2025-05-10 14:54:33,815 - INFO -  * Debugger PIN: 865-307-807
2025-05-10 14:55:39,125 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 14:55:40,273 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 14:55:40,275 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 14:55:40,277 - INFO - JAX version 0.5.3 available.
2025-05-10 14:55:46,208 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 14:55:50,565 - INFO - Embeddings model initialized successfully
2025-05-10 14:55:50,565 - INFO - Creating new vectorstore for language: en...
2025-05-10 14:55:50,818 - INFO - Loading faiss with AVX2 support.
2025-05-10 14:55:51,282 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 14:55:51,297 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 14:55:51,302 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 14:55:51,302 - INFO - Creating new vectorstore for language: hi...
2025-05-10 14:55:51,491 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 14:55:51,491 - INFO - Model preloading completed!
2025-05-10 14:56:57,237 - INFO - 127.0.0.1 - - [10/May/2025 14:56:57] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:56:57,490 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:56:57,490 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 14:56:57,490 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:56:57,490 - INFO - Found API key in environment variables
2025-05-10 14:56:57,999 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:56:58,002 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:56:59,582 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:56:59,623 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:56:59,627 - INFO - 127.0.0.1 - - [10/May/2025 14:56:59] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:14,141 - INFO - 127.0.0.1 - - [10/May/2025 14:59:14] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:14,390 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:59:14,390 - INFO - [PROCESSING] Question: Apps not working on my TV, Language: en
2025-05-10 14:59:14,390 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:59:14,390 - INFO - Found API key in environment variables
2025-05-10 14:59:14,821 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:59:14,821 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:59:16,057 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:59:16,063 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:59:16,063 - INFO - 127.0.0.1 - - [10/May/2025 14:59:16] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:31,918 - INFO - 127.0.0.1 - - [10/May/2025 14:59:31] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 14:59:32,223 - INFO - [REQUEST] Received FAQ request
2025-05-10 14:59:32,223 - INFO - [PROCESSING] Question: Wi-Fi issues on old Samsung TV, Language: en
2025-05-10 14:59:32,223 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 14:59:32,223 - INFO - Found API key in environment variables
2025-05-10 14:59:32,633 - INFO - [GROQ_API] API key verified successfully
2025-05-10 14:59:32,633 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 14:59:33,785 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    qa_chain = RetrievalQA.from_chain_type(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 14:59:33,785 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    result = qa_chain({"query": question})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    "error": str(e)
                 ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 14:59:33,785 - INFO - 127.0.0.1 - - [10/May/2025 14:59:33] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:29,142 - INFO - 127.0.0.1 - - [10/May/2025 15:00:29] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:29,451 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:00:29,451 - INFO - [PROCESSING] Question: m3164, Language: en
2025-05-10 15:00:29,451 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:00:29,451 - INFO - Found API key in environment variables
2025-05-10 15:00:29,922 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:00:29,924 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:00:31,214 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    from langchain_core.prompts import PromptTemplate
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 15:00:31,222 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    # Get QA Chain with the pre-trained vectorstore
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    # Create a robust fallback
                             ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 15:00:31,222 - INFO - 127.0.0.1 - - [10/May/2025 15:00:31] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:50,305 - INFO - 127.0.0.1 - - [10/May/2025 15:00:50] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:00:50,570 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:00:50,570 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:00:50,570 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:00:50,570 - INFO - Found API key in environment variables
2025-05-10 15:00:51,089 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:00:51,089 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:00:52,231 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 330, in get_qa_chain
    from langchain_core.prompts import PromptTemplate
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\retrieval_qa\base.py", line 115, in from_chain_type
    combine_documents_chain = load_qa_chain(
                              ^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 191, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 266, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain\chains\question_answering\chain.py", line 76, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\load\serializable.py", line 130, in __init__
    super().__init__(*args, **kwargs)
  File "C:\Python312\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for LLMChain
prompt
  Input should be a valid dictionary or instance of BasePromptTemplate [type=model_type, input_value='Given the following cont...: {question}\n\nAnswer:', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-05-10 15:00:52,231 - ERROR - [ERROR] cannot access free variable 'e' where it is not associated with a value in enclosing scope
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 414, in handle_faq
    # Get QA Chain with the pre-trained vectorstore
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 354, in __call__
    # Create a robust fallback
                             ^
NameError: cannot access free variable 'e' where it is not associated with a value in enclosing scope
2025-05-10 15:00:52,235 - INFO - 127.0.0.1 - - [10/May/2025 15:00:52] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:05,967 - INFO - Found API key in environment variables
2025-05-10 15:01:06,450 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:06,464 - INFO - Groq API key verified successfully!
2025-05-10 15:01:06,465 - INFO - Starting preload of models...
2025-05-10 15:01:06,465 - INFO - Initializing embeddings model...
2025-05-10 15:01:06,490 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:01:06,491 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:01:10,192 - INFO - 127.0.0.1 - - [10/May/2025 15:01:10] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:10,442 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:10,442 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:01:10,442 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:10,442 - INFO - Found API key in environment variables
2025-05-10 15:01:10,866 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:10,870 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:10,870 - INFO - Initializing embeddings model...
2025-05-10 15:01:26,907 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:01:27,739 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:01:27,739 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:01:27,750 - INFO - JAX version 0.5.3 available.
2025-05-10 15:01:29,261 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:01:29,270 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:01:32,838 - INFO - Embeddings model initialized successfully
2025-05-10 15:01:32,838 - INFO - Loading existing index from faiss_en_index...
2025-05-10 15:01:32,844 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:01:32,844 - INFO - Creating new vectorstore for language: en...
2025-05-10 15:01:32,949 - INFO - Loading faiss with AVX2 support.
2025-05-10 15:01:32,990 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 15:01:33,000 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 15:01:33,004 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 15:01:33,296 - INFO - Embeddings model initialized successfully
2025-05-10 15:01:33,296 - INFO - Loading existing index from faiss_hi_index...
2025-05-10 15:01:33,296 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:01:33,300 - INFO - Creating new vectorstore for language: hi...
2025-05-10 15:01:33,483 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 15:01:33,484 - INFO - Model preloading completed!
2025-05-10 15:01:34,508 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:35,135 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:35,151 - INFO - Request processed in 24.71 seconds
2025-05-10 15:01:35,151 - INFO - 127.0.0.1 - - [10/May/2025 15:01:35] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:46,663 - INFO - 127.0.0.1 - - [10/May/2025 15:01:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:46,972 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:46,972 - INFO - [PROCESSING] Question: my tv is not working, Language: en
2025-05-10 15:01:46,972 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:46,972 - INFO - Found API key in environment variables
2025-05-10 15:01:47,421 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:47,427 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:48,531 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:49,164 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:49,164 - INFO - Request processed in 2.19 seconds
2025-05-10 15:01:49,164 - INFO - 127.0.0.1 - - [10/May/2025 15:01:49] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:57,570 - INFO - 127.0.0.1 - - [10/May/2025 15:01:57] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:01:57,882 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:01:57,882 - INFO - [PROCESSING] Question: it is in safe mode, Language: en
2025-05-10 15:01:57,882 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:01:57,882 - INFO - Found API key in environment variables
2025-05-10 15:01:58,261 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:01:58,261 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:01:59,372 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:01:59,706 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:01:59,712 - INFO - Request processed in 1.83 seconds
2025-05-10 15:01:59,712 - INFO - 127.0.0.1 - - [10/May/2025 15:01:59] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:06:59,800 - INFO - Found API key in environment variables
2025-05-10 15:07:00,327 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:07:00,329 - INFO - Groq API key verified successfully!
2025-05-10 15:07:00,330 - INFO - Starting preload of models...
2025-05-10 15:07:00,330 - INFO - Initializing embeddings model...
2025-05-10 15:07:00,352 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:07:00,352 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:07:06,935 - INFO - 127.0.0.1 - - [10/May/2025 15:07:06] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:07:07,320 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:07:08,828 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:07:08,828 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:07:08,829 - INFO - Found API key in environment variables
2025-05-10 15:07:09,299 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:07:09,304 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:07:09,304 - INFO - Initializing embeddings model...
2025-05-10 15:07:22,467 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:07:23,018 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:07:23,021 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:07:23,024 - INFO - JAX version 0.5.3 available.
2025-05-10 15:07:24,570 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:24,572 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:27,831 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:27,873 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:27,873 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-10 15:07:27,882 - INFO - Initializing embeddings model...
2025-05-10 15:07:27,884 - ERROR - QA Chain is None - Using direct fallback
2025-05-10 15:07:27,884 - INFO - 127.0.0.1 - - [10/May/2025 15:07:27] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:07:27,890 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:30,482 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:30,497 - INFO - Initializing embeddings model...
2025-05-10 15:07:30,500 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:07:33,237 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 110, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-10 15:07:33,242 - INFO - Model preloading completed!
2025-05-10 15:09:04,935 - INFO - Found API key in environment variables
2025-05-10 15:09:05,446 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:09:05,447 - INFO - Groq API key verified successfully!
2025-05-10 15:09:05,447 - INFO - Starting preload of models...
2025-05-10 15:09:05,470 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-10 15:09:05,470 - INFO - [33mPress CTRL+C to quit[0m
2025-05-10 15:09:25,571 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-10 15:09:26,082 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-10 15:09:26,082 - INFO - TensorFlow version 2.19.0 available.
2025-05-10 15:09:26,095 - INFO - JAX version 0.5.3 available.
2025-05-10 15:09:27,914 - INFO - Initializing embeddings model...
2025-05-10 15:09:29,181 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:29,181 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:34,886 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\Downloads\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-10 15:09:34,901 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:34,901 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:37,709 - INFO - Fallback embeddings model initialized successfully
2025-05-10 15:09:37,709 - INFO - Loading existing index from faiss_en_index...
2025-05-10 15:09:37,709 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:09:37,709 - INFO - Creating new vectorstore for language: en...
2025-05-10 15:09:37,709 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:37,709 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:09:40,464 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:09:40,464 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-10 15:09:43,874 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-10 15:11:46,494 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-10 15:11:54,608 - INFO - Loading faiss with AVX2 support.
2025-05-10 15:11:54,970 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-10 15:11:54,987 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-10 15:11:54,992 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-10 15:11:54,993 - INFO - Loading existing index from faiss_hi_index...
2025-05-10 15:11:54,993 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-10 15:11:54,993 - INFO - Creating new vectorstore for language: hi...
2025-05-10 15:11:54,997 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:11:55,000 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-10 15:11:58,081 - INFO - Use pytorch device_name: cuda:0
2025-05-10 15:11:58,081 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-10 15:12:02,650 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-10 15:12:02,650 - INFO - Model preloading completed!
2025-05-10 15:35:23,729 - INFO - 127.0.0.1 - - [10/May/2025 15:35:23] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:23,990 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:35:24,657 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-10 15:35:24,657 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:35:24,657 - INFO - Found API key in environment variables
2025-05-10 15:35:25,123 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:35:25,123 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:35:26,490 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:35:27,120 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:35:27,132 - INFO - Request processed in 3.14 seconds
2025-05-10 15:35:27,132 - INFO - 127.0.0.1 - - [10/May/2025 15:35:27] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:39,750 - INFO - 127.0.0.1 - - [10/May/2025 15:35:39] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:35:40,061 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:35:40,066 - INFO - [PROCESSING] Question: lg tv is not working, Language: en
2025-05-10 15:35:40,066 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:35:40,066 - INFO - Found API key in environment variables
2025-05-10 15:35:41,000 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:35:41,000 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:35:42,211 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:35:42,815 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:35:42,820 - INFO - Request processed in 2.76 seconds
2025-05-10 15:35:42,822 - INFO - 127.0.0.1 - - [10/May/2025 15:35:42] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:24,748 - INFO - 127.0.0.1 - - [10/May/2025 15:36:24] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:25,051 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:36:25,078 - INFO - [PROCESSING] Question: TV starting in Safe Mode, Language: en
2025-05-10 15:36:25,078 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:36:25,079 - INFO - Found API key in environment variables
2025-05-10 15:36:25,448 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:36:25,449 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:36:26,650 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:36:26,988 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:36:26,995 - INFO - Request processed in 1.94 seconds
2025-05-10 15:36:26,995 - INFO - 127.0.0.1 - - [10/May/2025 15:36:26] "POST /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:36,776 - INFO - 127.0.0.1 - - [10/May/2025 15:36:36] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-10 15:36:37,082 - INFO - [REQUEST] Received FAQ request
2025-05-10 15:36:37,093 - INFO - [PROCESSING] Question: Wi-Fi issues on old Samsung TV, Language: en
2025-05-10 15:36:37,093 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-10 15:36:37,093 - INFO - Found API key in environment variables
2025-05-10 15:36:37,470 - INFO - [GROQ_API] API key verified successfully
2025-05-10 15:36:37,474 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-10 15:36:38,689 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-10 15:36:39,134 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-10 15:36:39,134 - INFO - Request processed in 2.05 seconds
2025-05-10 15:36:39,134 - INFO - 127.0.0.1 - - [10/May/2025 15:36:39] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:35:01,114 - INFO - Found API key in environment variables
2025-05-12 18:35:01,692 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:35:01,693 - INFO - Groq API key verified successfully!
2025-05-12 18:35:01,695 - INFO - Starting preload of models...
2025-05-12 18:35:01,766 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:35:01,766 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:35:46,429 - INFO - 127.0.0.1 - - [12/May/2025 18:35:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:35:46,701 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:35:48,976 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:35:48,976 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:35:48,976 - INFO - Found API key in environment variables
2025-05-12 18:35:49,579 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:35:49,581 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:36:15,746 - INFO - Found API key in environment variables
2025-05-12 18:36:16,193 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:36:16,193 - INFO - Groq API key verified successfully!
2025-05-12 18:36:16,195 - INFO - Starting preload of models...
2025-05-12 18:36:16,227 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:36:16,227 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:36:23,691 - INFO - 127.0.0.1 - - [12/May/2025 18:36:23] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:36:23,909 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:36:24,758 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:36:24,758 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:36:24,758 - INFO - Found API key in environment variables
2025-05-12 18:36:25,338 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:36:25,350 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:37:18,052 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-12 18:37:19,210 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-12 18:37:19,213 - INFO - TensorFlow version 2.19.0 available.
2025-05-12 18:37:19,242 - INFO - JAX version 0.5.3 available.
2025-05-12 18:37:26,839 - INFO - Initializing embeddings model...
2025-05-12 18:37:26,839 - INFO - Initializing embeddings model...
2025-05-12 18:37:28,214 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:28,214 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:28,217 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:28,217 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:33,852 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:33,913 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:33,913 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:33,948 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:33,954 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:33,954 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:36,627 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:36,683 - INFO - Initializing embeddings model...
2025-05-12 18:37:36,689 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:36,689 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:37,071 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:37,085 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:37:37,085 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:37:37,103 - INFO - 127.0.0.1 - - [12/May/2025 18:37:37] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:37:39,360 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:39,364 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:39,369 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:42,070 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:42,085 - INFO - Initializing embeddings model...
2025-05-12 18:37:42,093 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:42,093 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:44,780 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:44,787 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:37:44,787 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:37:49,036 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:37:49,042 - INFO - Model preloading completed!
2025-05-12 18:38:00,171 - INFO - 127.0.0.1 - - [12/May/2025 18:38:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:38:00,433 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:38:00,470 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:38:00,472 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:38:00,472 - INFO - Found API key in environment variables
2025-05-12 18:38:00,932 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:38:00,932 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:38:00,932 - INFO - Initializing embeddings model...
2025-05-12 18:38:00,941 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:38:00,941 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:38:04,189 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:38:04,199 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:38:04,199 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:38:06,967 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:38:06,984 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:38:06,984 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:38:06,986 - INFO - 127.0.0.1 - - [12/May/2025 18:38:06] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:45:48,749 - INFO - Found API key in environment variables
2025-05-12 18:45:49,157 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:45:49,157 - INFO - Groq API key verified successfully!
2025-05-12 18:45:49,160 - INFO - Starting preload of models...
2025-05-12 18:45:49,199 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 18:45:49,199 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 18:45:59,479 - INFO - 127.0.0.1 - - [12/May/2025 18:45:59] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:45:59,734 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:46:00,593 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 18:46:00,597 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:46:00,597 - INFO - Found API key in environment variables
2025-05-12 18:46:01,077 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:46:01,077 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:46:20,177 - WARNING - From C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2025-05-12 18:46:21,046 - INFO - PyTorch version 2.5.1+cu121 available.
2025-05-12 18:46:21,046 - INFO - TensorFlow version 2.19.0 available.
2025-05-12 18:46:21,046 - INFO - JAX version 0.5.3 available.
2025-05-12 18:46:23,482 - INFO - Initializing embeddings model...
2025-05-12 18:46:23,482 - INFO - Initializing embeddings model...
2025-05-12 18:46:24,732 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:24,735 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:24,735 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:24,735 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:28,844 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:28,844 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:28,867 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:28,870 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:28,870 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:28,870 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:31,421 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:31,436 - INFO - Initializing embeddings model...
2025-05-12 18:46:31,442 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:31,442 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:31,512 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:31,518 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:46:31,518 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:46:31,527 - INFO - 127.0.0.1 - - [12/May/2025 18:46:31] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 18:46:34,238 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:34,244 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:34,244 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:36,802 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:36,814 - INFO - Initializing embeddings model...
2025-05-12 18:46:36,820 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:36,820 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:39,457 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:39,464 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:46:39,464 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:46:42,199 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:46:42,219 - INFO - Model preloading completed!
2025-05-12 18:47:00,859 - INFO - 127.0.0.1 - - [12/May/2025 18:47:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 18:47:01,113 - INFO - [REQUEST] Received FAQ request
2025-05-12 18:47:01,126 - INFO - [PROCESSING] Question: tv not working, Language: en
2025-05-12 18:47:01,126 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 18:47:01,126 - INFO - Found API key in environment variables
2025-05-12 18:47:01,637 - INFO - [GROQ_API] API key verified successfully
2025-05-12 18:47:01,637 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 18:47:01,637 - INFO - Initializing embeddings model...
2025-05-12 18:47:01,644 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:47:01,644 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:47:04,921 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:47:04,931 - INFO - Use pytorch device_name: cuda:0
2025-05-12 18:47:04,932 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 18:47:08,031 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1333, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 18:47:08,044 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 18:47:08,044 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 18:47:08,049 - INFO - 127.0.0.1 - - [12/May/2025 18:47:08] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:22:02,629 - INFO - Found API key in environment variables
2025-05-12 19:22:03,175 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:22:03,177 - INFO - Groq API key verified successfully!
2025-05-12 19:22:03,179 - INFO - Starting preload of models...
2025-05-12 19:22:03,241 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:22:03,241 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:22:25,537 - INFO - Initializing embeddings model...
2025-05-12 19:22:25,541 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:25,541 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:30,343 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:22:30,361 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:30,361 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:33,407 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:22:33,407 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:22:33,407 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:33,407 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:22:33,416 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:33,416 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:36,209 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:36,209 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:41,256 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,332 - INFO - 127.0.0.1 - - [12/May/2025 19:22:41] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:22:41,377 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,586 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:22:41,861 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:41,861 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:41,861 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:22:41,863 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:41,863 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:22:41,867 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:41,867 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:43,932 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:22:43,932 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:22:43,932 - INFO - Found API key in environment variables
2025-05-12 19:22:44,345 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:22:44,347 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:22:44,347 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:22:44,347 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:22:44,347 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:22:44,356 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:44,356 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:22:45,482 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:45,487 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:47,380 - INFO - Use pytorch device_name: cpu
2025-05-12 19:22:47,380 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:22:50,166 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,361 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,590 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:50,590 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:50,590 - INFO - Model preloading completed!
2025-05-12 19:22:53,791 - WARNING - Vectorstore training failed with model client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:53,896 - WARNING - Vectorstore training failed with model model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:54,106 - WARNING - Vectorstore training failed with model model_name='paraphrase-multilingual-MiniLM-L12-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).
2025-05-12 19:22:54,106 - ERROR - Failed to train vectorstore with any available embeddings model
2025-05-12 19:22:54,106 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 19:22:54,109 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 19:22:54,110 - INFO - 127.0.0.1 - - [12/May/2025 19:22:54] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:23,692 - INFO - Found API key in environment variables
2025-05-12 19:25:24,195 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:25:24,201 - INFO - Groq API key verified successfully!
2025-05-12 19:25:24,203 - INFO - Starting preload of models...
2025-05-12 19:25:24,265 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:25:24,265 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:25:32,793 - INFO - 127.0.0.1 - - [12/May/2025 19:25:32] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:33,045 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:25:33,881 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:25:33,881 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:25:33,881 - INFO - Found API key in environment variables
2025-05-12 19:25:34,510 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:25:34,519 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:25:37,360 - INFO - Initializing embeddings model...
2025-05-12 19:25:37,360 - INFO - Initializing embeddings model...
2025-05-12 19:25:37,360 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:37,360 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:37,360 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:37,360 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:40,661 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:25:40,676 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:40,676 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:40,876 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:25:40,879 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:40,879 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:43,737 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:25:43,737 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:25:43,739 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:43,739 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:25:43,744 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:43,744 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:43,844 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:25:43,845 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:25:43,845 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:43,845 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:25:43,849 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:43,849 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:48,077 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:48,077 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:25:48,077 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:48,077 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:25:54,956 - INFO - Loading faiss with AVX2 support.
2025-05-12 19:25:55,090 - ERROR - [QA_CHAIN_CRITICAL_ERROR] Failed to create QA chain: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 397, in get_qa_chain
    vectorstore = get_vectorstore(language)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 206, in get_vectorstore
    return train_vectorstore(language)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 176, in train_vectorstore
    HuggingFaceEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:25:55,384 - INFO - Request processed in 22.34 seconds
2025-05-12 19:25:55,386 - INFO - 127.0.0.1 - - [12/May/2025 19:25:55] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:25:55,587 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-12 19:25:55,602 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-12 19:25:55,613 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-12 19:25:55,613 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:25:55,613 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:25:55,613 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:25:55,620 - INFO - Use pytorch device_name: cpu
2025-05-12 19:25:55,620 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:25:58,679 - ERROR - Error during model preloading: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 479, in preload_models
    get_vectorstore("hi")
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 206, in get_vectorstore
    return train_vectorstore(language)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 175, in train_vectorstore
    HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_huggingface\embeddings\huggingface.py", line 94, in __init__
    self._client = model_cls(
                   ^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:22,265 - INFO - Found API key in environment variables
2025-05-12 19:39:22,743 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:39:22,743 - INFO - Groq API key verified successfully!
2025-05-12 19:39:22,745 - INFO - Starting preload of models...
2025-05-12 19:39:22,788 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:39:22,788 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:39:30,480 - INFO - 127.0.0.1 - - [12/May/2025 19:39:30] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:39:30,741 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:39:31,561 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:39:31,563 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:39:31,563 - INFO - Found API key in environment variables
2025-05-12 19:39:31,993 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:39:31,993 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:39:35,861 - INFO - Initializing embeddings model...
2025-05-12 19:39:35,861 - INFO - Initializing embeddings model...
2025-05-12 19:39:35,865 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:35,865 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:35,865 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:35,867 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:40,033 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:39:40,049 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:40,049 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:40,236 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:40,241 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:40,241 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:43,006 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:43,023 - ERROR - [QA_CHAIN_ERROR] Failed to get vectorstore for language: en
2025-05-12 19:39:43,025 - ERROR - QA Chain is None - Using direct fallback
2025-05-12 19:39:43,026 - INFO - 127.0.0.1 - - [12/May/2025 19:39:43] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:39:43,039 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:43,053 - INFO - Initializing embeddings model...
2025-05-12 19:39:43,064 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:43,066 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:46,675 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:46,678 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:46,680 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:49,311 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:49,325 - INFO - Initializing embeddings model...
2025-05-12 19:39:49,328 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:49,328 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:52,176 - ERROR - ERROR: Failed to initialize embeddings model: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 114, in initialize_embeddings
    model = SentenceTransformer('all-MiniLM-L6-v2')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:52,176 - INFO - Use pytorch device_name: cpu
2025-05-12 19:39:52,176 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:39:55,326 - ERROR - ERROR: Fallback embeddings initialization failed: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 132, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_core\_api\deprecation.py", line 224, in warn_if_direct_instance
    return wrapped(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\langchain_community\embeddings\huggingface.py", line 92, in __init__
    self.client = sentence_transformers.SentenceTransformer(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\sentence_transformers\SentenceTransformer.py", line 348, in __init__
    self.to(device)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\venv\Lib\site-packages\torch\nn\modules\module.py", line 1348, in convert
    raise NotImplementedError(
NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.
2025-05-12 19:39:55,338 - INFO - Model preloading completed!
2025-05-12 19:41:16,686 - INFO - Found API key in environment variables
2025-05-12 19:41:17,216 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:41:17,216 - INFO - Groq API key verified successfully!
2025-05-12 19:41:17,216 - INFO - Starting preload of models...
2025-05-12 19:41:17,272 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-12 19:41:17,272 - INFO - [33mPress CTRL+C to quit[0m
2025-05-12 19:41:28,374 - INFO - Initializing embeddings model...
2025-05-12 19:41:28,376 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:28,376 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:31,778 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 118, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-12 19:41:31,794 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:31,797 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:34,350 - INFO - Fallback embeddings model initialized successfully
2025-05-12 19:41:34,352 - INFO - Loading existing index from faiss_en_index...
2025-05-12 19:41:34,354 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:41:34,354 - INFO - Creating new vectorstore for language: en...
2025-05-12 19:41:34,357 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:34,357 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:37,265 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:37,265 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:41:41,423 - INFO - 127.0.0.1 - - [12/May/2025 19:41:41] "OPTIONS /api/complaints HTTP/1.1" 200 -
2025-05-12 19:41:41,462 - INFO - 127.0.0.1 - - [12/May/2025 19:41:41] "POST /api/complaints HTTP/1.1" 200 -
2025-05-12 19:41:42,202 - INFO - Loading faiss with AVX2 support.
2025-05-12 19:41:42,272 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-12 19:41:42,294 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-12 19:41:42,299 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-12 19:41:42,300 - INFO - Loading existing index from faiss_hi_index...
2025-05-12 19:41:42,301 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-12 19:41:42,301 - INFO - Creating new vectorstore for language: hi...
2025-05-12 19:41:42,308 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:42,308 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-12 19:41:44,992 - INFO - Use pytorch device_name: cpu
2025-05-12 19:41:44,992 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-12 19:41:49,815 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-12 19:41:49,815 - INFO - Model preloading completed!
2025-05-12 19:42:00,689 - INFO - 127.0.0.1 - - [12/May/2025 19:42:00] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:00,945 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:02,450 - INFO - [PROCESSING] Question: tv not working in safe mode, Language: en
2025-05-12 19:42:02,450 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:02,450 - INFO - Found API key in environment variables
2025-05-12 19:42:02,963 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:02,963 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:04,834 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:05,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:05,350 - INFO - Request processed in 4.40 seconds
2025-05-12 19:42:05,364 - INFO - 127.0.0.1 - - [12/May/2025 19:42:05] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:18,118 - INFO - 127.0.0.1 - - [12/May/2025 19:42:18] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:18,432 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:18,437 - INFO - [PROCESSING] Question: lg tv not working, Language: en
2025-05-12 19:42:18,437 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:18,437 - INFO - Found API key in environment variables
2025-05-12 19:42:18,824 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:18,825 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:20,434 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:20,835 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:20,843 - INFO - Request processed in 2.41 seconds
2025-05-12 19:42:20,843 - INFO - 127.0.0.1 - - [12/May/2025 19:42:20] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:39,836 - INFO - 127.0.0.1 - - [12/May/2025 19:42:39] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:40,152 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:40,188 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-12 19:42:40,188 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:40,188 - INFO - Found API key in environment variables
2025-05-12 19:42:40,645 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:40,645 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:42,112 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:42,673 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:42,676 - INFO - Request processed in 2.52 seconds
2025-05-12 19:42:42,679 - INFO - 127.0.0.1 - - [12/May/2025 19:42:42] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:50,466 - INFO - 127.0.0.1 - - [12/May/2025 19:42:50] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:42:50,770 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:42:50,776 - INFO - [PROCESSING] Question: only samsung, Language: en
2025-05-12 19:42:50,776 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:42:50,776 - INFO - Found API key in environment variables
2025-05-12 19:42:51,149 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:42:51,151 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:42:52,716 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:42:52,992 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:42:52,994 - INFO - Request processed in 2.22 seconds
2025-05-12 19:42:52,994 - INFO - 127.0.0.1 - - [12/May/2025 19:42:52] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:43:46,795 - INFO - 127.0.0.1 - - [12/May/2025 19:43:46] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:43:47,108 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:43:47,130 - INFO - [PROCESSING] Question: nokia mobile not working, Language: en
2025-05-12 19:43:47,130 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:43:47,133 - INFO - Found API key in environment variables
2025-05-12 19:43:47,616 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:43:47,616 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:43:49,229 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:43:49,838 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:43:49,841 - INFO - Request processed in 2.73 seconds
2025-05-12 19:43:49,841 - INFO - 127.0.0.1 - - [12/May/2025 19:43:49] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:11,057 - INFO - 127.0.0.1 - - [12/May/2025 19:44:11] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:11,373 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:44:11,382 - INFO - [PROCESSING] Question: samsung mobile not working, Language: en
2025-05-12 19:44:11,382 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:44:11,382 - INFO - Found API key in environment variables
2025-05-12 19:44:11,790 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:44:11,793 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:44:13,185 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:44:13,725 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:44:13,728 - INFO - Request processed in 2.35 seconds
2025-05-12 19:44:13,728 - INFO - 127.0.0.1 - - [12/May/2025 19:44:13] "POST /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:29,283 - INFO - 127.0.0.1 - - [12/May/2025 19:44:29] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-12 19:44:29,601 - INFO - [REQUEST] Received FAQ request
2025-05-12 19:44:29,607 - INFO - [PROCESSING] Question: lg mobile not working, Language: en
2025-05-12 19:44:29,613 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-12 19:44:29,613 - INFO - Found API key in environment variables
2025-05-12 19:44:30,108 - INFO - [GROQ_API] API key verified successfully
2025-05-12 19:44:30,108 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-12 19:44:31,697 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-12 19:44:32,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-12 19:44:32,465 - INFO - Request processed in 2.86 seconds
2025-05-12 19:44:32,468 - INFO - 127.0.0.1 - - [12/May/2025 19:44:32] "POST /api/faq HTTP/1.1" 200 -
2025-05-13 14:13:04,277 - INFO - Found API key in environment variables
2025-05-13 14:13:04,698 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:13:04,698 - INFO - Groq API key verified successfully!
2025-05-13 14:13:04,698 - INFO - Starting preload of models...
2025-05-13 14:13:04,757 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:13:04,757 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:13:12,096 - INFO - 127.0.0.1 - - [13/May/2025 14:13:12] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-13 14:13:12,355 - INFO - [REQUEST] Received FAQ request
2025-05-13 14:13:12,355 - INFO - [PROCESSING] Question: hello, Language: en
2025-05-13 14:13:12,358 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:13:12,358 - INFO - Found API key in environment variables
2025-05-13 14:13:12,740 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:13:12,740 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:13:46,656 - INFO - Initializing embeddings model...
2025-05-13 14:13:46,656 - INFO - Initializing embeddings model...
2025-05-13 14:13:46,656 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:46,656 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:46,656 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:46,656 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:50,624 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 126, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:13:50,639 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:50,639 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:50,992 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 126, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:13:50,992 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:50,995 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:53,558 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:13:53,558 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:13:53,558 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:13:53,558 - INFO - Creating new vectorstore for language: en...
2025-05-13 14:13:53,566 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:53,568 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:54,239 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:13:54,240 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:13:54,240 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:13:54,240 - INFO - Creating new vectorstore for language: en...
2025-05-13 14:13:54,241 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:54,241 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:13:56,310 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:56,310 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:13:57,184 - INFO - Use pytorch device_name: cpu
2025-05-13 14:13:57,190 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:14:01,063 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:14:02,893 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:14:02,905 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:14:02,912 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-13 14:14:03,480 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-13 14:14:03,480 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:14:03,482 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:14:03,482 - INFO - Creating new vectorstore for language: hi...
2025-05-13 14:14:03,483 - INFO - Use pytorch device_name: cpu
2025-05-13 14:14:03,483 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:14:04,395 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:14:04,906 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:14:04,921 - INFO - Request processed in 52.57 seconds
2025-05-13 14:14:04,921 - INFO - 127.0.0.1 - - [13/May/2025 14:14:04] "POST /api/faq HTTP/1.1" 200 -
2025-05-13 14:14:06,465 - INFO - Use pytorch device_name: cpu
2025-05-13 14:14:06,475 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:14:10,693 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-13 14:14:10,693 - INFO - Model preloading completed!
2025-05-13 14:15:41,621 - INFO - 127.0.0.1 - - [13/May/2025 14:15:41] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-13 14:15:41,877 - INFO - [REQUEST] Received FAQ request
2025-05-13 14:15:41,877 - INFO - [PROCESSING] Question: samsung tv not working, Language: en
2025-05-13 14:15:41,877 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:15:41,877 - INFO - Found API key in environment variables
2025-05-13 14:15:42,285 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:15:42,285 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:15:43,370 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:15:43,886 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:15:43,886 - INFO - Request processed in 2.01 seconds
2025-05-13 14:15:43,886 - INFO - 127.0.0.1 - - [13/May/2025 14:15:43] "POST /api/faq HTTP/1.1" 200 -
2025-05-13 14:15:51,360 - INFO - 127.0.0.1 - - [13/May/2025 14:15:51] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-13 14:15:51,669 - INFO - [REQUEST] Received FAQ request
2025-05-13 14:15:51,669 - INFO - [PROCESSING] Question: lg tv not working, Language: en
2025-05-13 14:15:51,669 - INFO - 127.0.0.1 - - [13/May/2025 14:15:51] "POST /api/faq HTTP/1.1" 200 -
2025-05-13 14:16:11,190 - INFO - 127.0.0.1 - - [13/May/2025 14:16:11] "OPTIONS /api/faq HTTP/1.1" 200 -
2025-05-13 14:16:11,500 - INFO - [REQUEST] Received FAQ request
2025-05-13 14:16:11,500 - INFO - [PROCESSING] Question: i would like to register a complaint, Language: en
2025-05-13 14:16:11,500 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:16:11,500 - INFO - Found API key in environment variables
2025-05-13 14:16:11,882 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:16:11,887 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:16:12,930 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:16:13,600 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:16:13,602 - INFO - Request processed in 2.10 seconds
2025-05-13 14:16:13,603 - INFO - 127.0.0.1 - - [13/May/2025 14:16:13] "POST /api/faq HTTP/1.1" 200 -
2025-05-13 14:25:56,919 - INFO - Found API key in environment variables
2025-05-13 14:25:57,346 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:25:57,346 - INFO - Groq API key verified successfully!
2025-05-13 14:25:57,346 - INFO - Starting preload of models...
2025-05-13 14:25:57,388 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:25:57,388 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:26:07,011 - INFO - Initializing embeddings model...
2025-05-13 14:26:07,013 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:07,013 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:26:10,121 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 168, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:26:10,136 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:10,136 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:26:12,695 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:26:12,695 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:26:12,695 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:26:12,695 - INFO - Creating new vectorstore for language: en...
2025-05-13 14:26:12,701 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:12,701 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:26:15,271 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:15,276 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:26:19,569 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:26:19,647 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:26:19,661 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:26:19,666 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-13 14:26:19,666 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:26:19,669 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:26:19,669 - INFO - Creating new vectorstore for language: hi...
2025-05-13 14:26:19,672 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:19,672 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:26:22,280 - INFO - Use pytorch device_name: cpu
2025-05-13 14:26:22,280 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:26:27,104 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-13 14:26:27,104 - INFO - Model preloading completed!
2025-05-13 14:30:08,216 - INFO - Found API key in environment variables
2025-05-13 14:30:08,626 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:30:08,626 - INFO - Groq API key verified successfully!
2025-05-13 14:30:08,629 - INFO - Starting preload of models...
2025-05-13 14:30:08,669 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:30:08,670 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:30:18,116 - INFO - Initializing embeddings model...
2025-05-13 14:30:18,118 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:18,119 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:30:21,676 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:30:21,697 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:21,698 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:30:24,233 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:30:24,248 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:30:24,248 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:30:24,248 - INFO - Creating new vectorstore for language: en...
2025-05-13 14:30:24,248 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:24,248 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:30:26,899 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:26,899 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:30:30,934 - INFO - 127.0.0.1 - - [13/May/2025 14:30:30] "[33mOPTIONS /chat HTTP/1.1[0m" 404 -
2025-05-13 14:30:31,070 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:30:31,105 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:30:31,139 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:30:31,144 - INFO - Successfully trained and saved vectorstore for en at faiss_en_index
2025-05-13 14:30:31,146 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:30:31,146 - ERROR - Error loading existing index: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).. Will retrain.
2025-05-13 14:30:31,147 - INFO - Creating new vectorstore for language: hi...
2025-05-13 14:30:31,147 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:31,147 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:30:31,192 - INFO - 127.0.0.1 - - [13/May/2025 14:30:31] "[33mOPTIONS /chat HTTP/1.1[0m" 404 -
2025-05-13 14:30:35,240 - INFO - Use pytorch device_name: cpu
2025-05-13 14:30:35,240 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2
2025-05-13 14:30:36,452 - INFO - 127.0.0.1 - - [13/May/2025 14:30:36] "[33mOPTIONS /chat HTTP/1.1[0m" 404 -
2025-05-13 14:30:36,766 - INFO - 127.0.0.1 - - [13/May/2025 14:30:36] "[33mOPTIONS /chat HTTP/1.1[0m" 404 -
2025-05-13 14:30:39,620 - INFO - Successfully trained and saved vectorstore for hi at faiss_hi_index
2025-05-13 14:30:39,620 - INFO - Model preloading completed!
2025-05-13 14:36:05,659 - INFO - Found API key in environment variables
2025-05-13 14:36:06,077 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:36:06,077 - INFO - Groq API key verified successfully!
2025-05-13 14:36:06,077 - INFO - Starting preload of models...
2025-05-13 14:36:06,120 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:36:06,120 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:36:15,187 - INFO - Initializing embeddings model...
2025-05-13 14:36:15,188 - INFO - Use pytorch device_name: cpu
2025-05-13 14:36:15,188 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:36:18,681 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:36:18,693 - INFO - Use pytorch device_name: cpu
2025-05-13 14:36:18,693 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:36:21,304 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:36:21,304 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:36:21,307 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:36:21,342 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:36:21,350 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:36:21,371 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 14:36:21,371 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:36:21,382 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 14:36:21,382 - INFO - Model preloading completed!
2025-05-13 14:36:21,911 - INFO - 127.0.0.1 - - [13/May/2025 14:36:21] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:36:27,520 - INFO - 127.0.0.1 - - [13/May/2025 14:36:27] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
2025-05-13 14:36:27,525 - INFO - 127.0.0.1 - - [13/May/2025 14:36:27] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:36:27,851 - INFO - 127.0.0.1 - - [13/May/2025 14:36:27] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
2025-05-13 14:37:09,020 - INFO - 127.0.0.1 - - [13/May/2025 14:37:09] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:37:11,164 - INFO - 127.0.0.1 - - [13/May/2025 14:37:11] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
2025-05-13 14:37:12,454 - INFO - 127.0.0.1 - - [13/May/2025 14:37:12] "[35m[1mPOST /chat HTTP/1.1[0m" 500 -
2025-05-13 14:38:48,859 - INFO - Found API key in environment variables
2025-05-13 14:38:49,282 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:38:49,282 - INFO - Groq API key verified successfully!
2025-05-13 14:38:49,286 - INFO - Starting preload of models...
2025-05-13 14:38:49,327 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:38:49,327 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:38:58,498 - INFO - Initializing embeddings model...
2025-05-13 14:38:58,502 - INFO - Use pytorch device_name: cpu
2025-05-13 14:38:58,502 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:38:59,291 - INFO - 127.0.0.1 - - [13/May/2025 14:38:59] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:38:59,547 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:38:59,547 - INFO - Found API key in environment variables
2025-05-13 14:38:59,976 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:38:59,976 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:38:59,976 - INFO - Initializing embeddings model...
2025-05-13 14:38:59,978 - INFO - Use pytorch device_name: cpu
2025-05-13 14:38:59,978 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:39:01,579 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:39:01,595 - INFO - Use pytorch device_name: cpu
2025-05-13 14:39:01,595 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:39:03,277 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:39:03,280 - INFO - Use pytorch device_name: cpu
2025-05-13 14:39:03,280 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:39:04,166 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:39:04,166 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:39:04,171 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:39:04,202 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:39:04,219 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:39:04,220 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 14:39:04,220 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:39:04,222 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 14:39:04,223 - INFO - Model preloading completed!
2025-05-13 14:39:05,844 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:39:05,844 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:39:05,844 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 14:39:07,207 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:39:07,765 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:39:07,778 - INFO - Request processed in 8.23 seconds
2025-05-13 14:39:07,779 - INFO - 127.0.0.1 - - [13/May/2025 14:39:07] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:39:24,969 - INFO - 127.0.0.1 - - [13/May/2025 14:39:24] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:39:29,542 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:39:29,542 - INFO - Found API key in environment variables
2025-05-13 14:39:29,937 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:39:29,937 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:39:30,960 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:39:31,344 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:39:31,350 - INFO - Request processed in 1.81 seconds
2025-05-13 14:39:31,350 - INFO - 127.0.0.1 - - [13/May/2025 14:39:31] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:39:31,677 - INFO - 127.0.0.1 - - [13/May/2025 14:39:31] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:39:31,916 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:39:31,916 - INFO - Found API key in environment variables
2025-05-13 14:39:32,359 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:39:32,359 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:39:33,382 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:39:33,777 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:39:33,777 - INFO - Request processed in 1.86 seconds
2025-05-13 14:39:33,777 - INFO - 127.0.0.1 - - [13/May/2025 14:39:33] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:39:46,244 - INFO - 127.0.0.1 - - [13/May/2025 14:39:46] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:39:51,004 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:39:51,004 - INFO - Found API key in environment variables
2025-05-13 14:39:51,383 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:39:51,383 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:39:52,427 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:39:52,849 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:39:52,851 - INFO - Request processed in 1.85 seconds
2025-05-13 14:39:52,852 - INFO - 127.0.0.1 - - [13/May/2025 14:39:52] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:39:53,172 - INFO - 127.0.0.1 - - [13/May/2025 14:39:53] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:39:53,426 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:39:53,426 - INFO - Found API key in environment variables
2025-05-13 14:39:53,810 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:39:53,810 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:39:54,853 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:39:55,288 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:39:55,292 - INFO - Request processed in 1.87 seconds
2025-05-13 14:39:55,293 - INFO - 127.0.0.1 - - [13/May/2025 14:39:55] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:40:08,137 - INFO - 127.0.0.1 - - [13/May/2025 14:40:08] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:40:12,446 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:40:12,446 - INFO - Found API key in environment variables
2025-05-13 14:40:12,877 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:40:12,877 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:40:13,915 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:40:14,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:40:14,348 - INFO - Request processed in 1.90 seconds
2025-05-13 14:40:14,348 - INFO - 127.0.0.1 - - [13/May/2025 14:40:14] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:40:14,662 - INFO - 127.0.0.1 - - [13/May/2025 14:40:14] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:40:14,905 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:40:14,905 - INFO - Found API key in environment variables
2025-05-13 14:40:15,299 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:40:15,299 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:40:16,334 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:40:16,771 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:40:16,773 - INFO - Request processed in 1.87 seconds
2025-05-13 14:40:16,773 - INFO - 127.0.0.1 - - [13/May/2025 14:40:16] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:45:56,036 - INFO - 127.0.0.1 - - [13/May/2025 14:45:56] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:46:01,386 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:46:01,386 - INFO - Found API key in environment variables
2025-05-13 14:46:01,869 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:46:01,870 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:46:02,894 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:46:03,228 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:46:03,231 - INFO - Request processed in 1.85 seconds
2025-05-13 14:46:03,231 - INFO - 127.0.0.1 - - [13/May/2025 14:46:03] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:46:03,237 - INFO - 127.0.0.1 - - [13/May/2025 14:46:03] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:46:03,544 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:46:03,544 - INFO - Found API key in environment variables
2025-05-13 14:46:03,943 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:46:03,943 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:46:04,954 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:46:05,370 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:46:05,374 - INFO - Request processed in 1.83 seconds
2025-05-13 14:46:05,374 - INFO - 127.0.0.1 - - [13/May/2025 14:46:05] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:46:14,723 - INFO - 127.0.0.1 - - [13/May/2025 14:46:14] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:46:18,704 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:46:18,704 - INFO - Found API key in environment variables
2025-05-13 14:46:19,082 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:46:19,082 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:46:20,116 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:46:20,919 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:46:20,921 - INFO - Request processed in 2.22 seconds
2025-05-13 14:46:20,921 - INFO - 127.0.0.1 - - [13/May/2025 14:46:20] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:46:20,927 - INFO - 127.0.0.1 - - [13/May/2025 14:46:20] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:46:21,231 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:46:21,231 - INFO - Found API key in environment variables
2025-05-13 14:46:21,620 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:46:21,620 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:46:22,659 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:46:23,110 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:46:23,114 - INFO - Request processed in 1.88 seconds
2025-05-13 14:46:23,114 - INFO - 127.0.0.1 - - [13/May/2025 14:46:23] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:49:02,104 - INFO - Found API key in environment variables
2025-05-13 14:49:02,499 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:49:02,499 - INFO - Groq API key verified successfully!
2025-05-13 14:49:02,499 - INFO - Starting preload of models...
2025-05-13 14:49:02,544 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:49:02,544 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:49:11,259 - INFO - Initializing embeddings model...
2025-05-13 14:49:11,259 - INFO - Use pytorch device_name: cpu
2025-05-13 14:49:11,259 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:49:14,232 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:49:14,243 - INFO - Use pytorch device_name: cpu
2025-05-13 14:49:14,243 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:49:16,754 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:49:16,754 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:49:16,760 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:49:16,792 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:49:16,803 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:49:16,804 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 14:49:16,805 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:49:16,806 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 14:49:16,806 - INFO - Model preloading completed!
2025-05-13 14:55:06,754 - INFO - Found API key in environment variables
2025-05-13 14:55:07,199 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:55:07,200 - INFO - Groq API key verified successfully!
2025-05-13 14:55:07,201 - INFO - Starting preload of models...
2025-05-13 14:55:07,242 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 14:55:07,242 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 14:55:16,471 - INFO - Initializing embeddings model...
2025-05-13 14:55:16,472 - INFO - Use pytorch device_name: cpu
2025-05-13 14:55:16,473 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:55:20,792 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 14:55:20,804 - INFO - Use pytorch device_name: cpu
2025-05-13 14:55:20,804 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 14:55:23,533 - INFO - Fallback embeddings model initialized successfully
2025-05-13 14:55:23,533 - INFO - Loading existing index from faiss_en_index...
2025-05-13 14:55:23,544 - INFO - Loading faiss with AVX2 support.
2025-05-13 14:55:23,570 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 14:55:23,583 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 14:55:23,585 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 14:55:23,585 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 14:55:23,586 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 14:55:23,587 - INFO - Model preloading completed!
2025-05-13 14:55:32,870 - INFO - 127.0.0.1 - - [13/May/2025 14:55:32] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:55:33,134 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:55:33,134 - INFO - Found API key in environment variables
2025-05-13 14:55:33,634 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:55:33,634 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:55:35,320 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:55:36,014 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:55:36,027 - INFO - Request processed in 2.89 seconds
2025-05-13 14:55:36,029 - INFO - 127.0.0.1 - - [13/May/2025 14:55:36] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:55:45,990 - INFO - 127.0.0.1 - - [13/May/2025 14:55:45] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:55:51,989 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:55:51,989 - INFO - Found API key in environment variables
2025-05-13 14:55:52,434 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:55:52,434 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:55:53,454 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:55:53,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:55:53,854 - INFO - Request processed in 1.87 seconds
2025-05-13 14:55:53,854 - INFO - 127.0.0.1 - - [13/May/2025 14:55:53] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:55:54,166 - INFO - 127.0.0.1 - - [13/May/2025 14:55:54] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:55:54,419 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:55:54,419 - INFO - Found API key in environment variables
2025-05-13 14:55:54,854 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:55:54,854 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:55:55,881 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:55:56,356 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:55:56,358 - INFO - Request processed in 1.94 seconds
2025-05-13 14:55:56,359 - INFO - 127.0.0.1 - - [13/May/2025 14:55:56] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:56:05,042 - INFO - 127.0.0.1 - - [13/May/2025 14:56:05] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:56:09,689 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:56:09,689 - INFO - Found API key in environment variables
2025-05-13 14:56:10,100 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:56:10,100 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:56:11,132 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:56:11,497 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:56:11,497 - INFO - Request processed in 1.81 seconds
2025-05-13 14:56:11,497 - INFO - 127.0.0.1 - - [13/May/2025 14:56:11] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:56:11,816 - INFO - 127.0.0.1 - - [13/May/2025 14:56:11] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:56:12,081 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:56:12,081 - INFO - Found API key in environment variables
2025-05-13 14:56:12,456 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:56:12,456 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:56:13,480 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:56:13,885 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:56:13,900 - INFO - Request processed in 1.82 seconds
2025-05-13 14:56:13,900 - INFO - 127.0.0.1 - - [13/May/2025 14:56:13] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:56:19,210 - INFO - 127.0.0.1 - - [13/May/2025 14:56:19] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:56:25,219 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:56:25,220 - INFO - Found API key in environment variables
2025-05-13 14:56:25,681 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:56:25,681 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:56:26,716 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:56:27,104 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:56:27,104 - INFO - Request processed in 1.88 seconds
2025-05-13 14:56:27,104 - INFO - 127.0.0.1 - - [13/May/2025 14:56:27] "POST /chat HTTP/1.1" 200 -
2025-05-13 14:56:27,416 - INFO - 127.0.0.1 - - [13/May/2025 14:56:27] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 14:56:27,670 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 14:56:27,670 - INFO - Found API key in environment variables
2025-05-13 14:56:28,069 - INFO - [GROQ_API] API key verified successfully
2025-05-13 14:56:28,069 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 14:56:29,098 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 14:56:29,478 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 14:56:29,484 - INFO - Request processed in 1.81 seconds
2025-05-13 14:56:29,484 - INFO - 127.0.0.1 - - [13/May/2025 14:56:29] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:00:27,545 - INFO - Found API key in environment variables
2025-05-13 15:00:27,992 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:00:27,992 - INFO - Groq API key verified successfully!
2025-05-13 15:00:27,992 - INFO - Starting preload of models...
2025-05-13 15:00:28,034 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 15:00:28,034 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 15:00:36,660 - INFO - Initializing embeddings model...
2025-05-13 15:00:36,660 - INFO - Use pytorch device_name: cpu
2025-05-13 15:00:36,660 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:00:40,084 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 15:00:40,095 - INFO - Use pytorch device_name: cpu
2025-05-13 15:00:40,095 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:00:42,864 - INFO - Fallback embeddings model initialized successfully
2025-05-13 15:00:42,865 - INFO - Loading existing index from faiss_en_index...
2025-05-13 15:00:42,869 - INFO - Loading faiss with AVX2 support.
2025-05-13 15:00:42,902 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 15:00:42,913 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 15:00:42,915 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 15:00:42,915 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 15:00:42,916 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 15:00:42,916 - INFO - Model preloading completed!
2025-05-13 15:00:50,479 - INFO - 127.0.0.1 - - [13/May/2025 15:00:50] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:00:54,540 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:00:54,540 - INFO - Found API key in environment variables
2025-05-13 15:00:54,954 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:00:54,954 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:00:56,110 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:00:56,110 - INFO - Found API key in environment variables
2025-05-13 15:00:56,242 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:00:56,537 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:00:56,538 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:00:56,751 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:00:56,756 - INFO - Request processed in 2.22 seconds
2025-05-13 15:00:56,756 - INFO - 127.0.0.1 - - [13/May/2025 15:00:56] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:00:57,556 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:00:57,942 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:00:57,944 - INFO - Request processed in 1.83 seconds
2025-05-13 15:00:57,945 - INFO - 127.0.0.1 - - [13/May/2025 15:00:57] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:00:58,249 - INFO - 127.0.0.1 - - [13/May/2025 15:00:58] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:00:58,504 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:00:58,505 - INFO - Found API key in environment variables
2025-05-13 15:00:58,931 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:00:58,932 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:00:59,966 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:01:00,353 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:01:00,353 - INFO - Request processed in 1.85 seconds
2025-05-13 15:01:00,353 - INFO - 127.0.0.1 - - [13/May/2025 15:01:00] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:01:14,090 - INFO - 127.0.0.1 - - [13/May/2025 15:01:14] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:01:18,634 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:01:18,634 - INFO - Found API key in environment variables
2025-05-13 15:01:19,040 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:01:19,040 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:01:20,070 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:01:20,499 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:01:20,499 - INFO - Request processed in 1.87 seconds
2025-05-13 15:01:20,499 - INFO - 127.0.0.1 - - [13/May/2025 15:01:20] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:01:20,812 - INFO - 127.0.0.1 - - [13/May/2025 15:01:20] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:01:21,077 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:01:21,077 - INFO - Found API key in environment variables
2025-05-13 15:01:21,504 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:01:21,504 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:01:22,549 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:01:22,979 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:01:22,981 - INFO - Request processed in 1.90 seconds
2025-05-13 15:01:22,982 - INFO - 127.0.0.1 - - [13/May/2025 15:01:22] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:01:30,160 - INFO - 127.0.0.1 - - [13/May/2025 15:01:30] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:01:39,469 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:01:39,469 - INFO - Found API key in environment variables
2025-05-13 15:01:39,854 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:01:39,854 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:01:40,928 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:01:41,308 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:01:41,310 - INFO - Request processed in 1.84 seconds
2025-05-13 15:01:41,312 - INFO - 127.0.0.1 - - [13/May/2025 15:01:41] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:01:41,316 - INFO - 127.0.0.1 - - [13/May/2025 15:01:41] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:01:41,622 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:01:41,622 - INFO - Found API key in environment variables
2025-05-13 15:01:41,977 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:01:41,977 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:01:43,016 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:01:43,436 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:01:43,438 - INFO - Request processed in 1.82 seconds
2025-05-13 15:01:43,440 - INFO - 127.0.0.1 - - [13/May/2025 15:01:43] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:15:28,832 - INFO - Found API key in environment variables
2025-05-13 15:15:29,236 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:15:29,236 - INFO - Groq API key verified successfully!
2025-05-13 15:15:29,236 - INFO - Starting preload of models...
2025-05-13 15:15:29,287 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 15:15:29,288 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 15:15:39,361 - INFO - Initializing embeddings model...
2025-05-13 15:15:39,361 - INFO - Use pytorch device_name: cpu
2025-05-13 15:15:39,361 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:15:44,049 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 15:15:44,066 - INFO - Use pytorch device_name: cpu
2025-05-13 15:15:44,066 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:15:46,659 - INFO - Fallback embeddings model initialized successfully
2025-05-13 15:15:46,660 - INFO - Loading existing index from faiss_en_index...
2025-05-13 15:15:46,665 - INFO - Loading faiss with AVX2 support.
2025-05-13 15:15:46,696 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 15:15:46,708 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 15:15:46,708 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 15:15:46,708 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 15:15:46,708 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 15:15:46,708 - INFO - Model preloading completed!
2025-05-13 15:15:52,491 - INFO - 127.0.0.1 - - [13/May/2025 15:15:52] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:15:59,847 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:15:59,848 - INFO - Found API key in environment variables
2025-05-13 15:16:00,248 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:16:00,248 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:16:01,553 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:16:02,070 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:16:02,079 - INFO - Request processed in 2.23 seconds
2025-05-13 15:16:02,079 - INFO - 127.0.0.1 - - [13/May/2025 15:16:02] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:16:02,085 - INFO - 127.0.0.1 - - [13/May/2025 15:16:02] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:16:02,392 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:16:02,392 - INFO - Found API key in environment variables
2025-05-13 15:16:02,804 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:16:02,804 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:16:03,822 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:16:04,248 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:16:04,254 - INFO - Request processed in 1.86 seconds
2025-05-13 15:16:04,254 - INFO - 127.0.0.1 - - [13/May/2025 15:16:04] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:20:34,040 - INFO - Found API key in environment variables
2025-05-13 15:20:34,503 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:20:34,503 - INFO - Groq API key verified successfully!
2025-05-13 15:20:34,503 - INFO - Starting preload of models...
2025-05-13 15:20:34,547 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-13 15:20:34,547 - INFO - [33mPress CTRL+C to quit[0m
2025-05-13 15:20:43,536 - INFO - Initializing embeddings model...
2025-05-13 15:20:43,537 - INFO - Use pytorch device_name: cpu
2025-05-13 15:20:43,538 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:20:47,423 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-13 15:20:47,432 - INFO - Use pytorch device_name: cpu
2025-05-13 15:20:47,432 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-13 15:20:51,228 - INFO - Fallback embeddings model initialized successfully
2025-05-13 15:20:51,228 - INFO - Loading existing index from faiss_en_index...
2025-05-13 15:20:51,234 - INFO - Loading faiss with AVX2 support.
2025-05-13 15:20:51,266 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-13 15:20:51,277 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-13 15:20:51,277 - INFO - Successfully loaded existing vectorstore for en
2025-05-13 15:20:51,277 - INFO - Loading existing index from faiss_hi_index...
2025-05-13 15:20:51,277 - INFO - Successfully loaded existing vectorstore for hi
2025-05-13 15:20:51,277 - INFO - Model preloading completed!
2025-05-13 15:20:59,484 - INFO - 127.0.0.1 - - [13/May/2025 15:20:59] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:20:59,737 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:20:59,737 - INFO - Found API key in environment variables
2025-05-13 15:21:00,122 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:21:00,122 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:21:01,429 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:21:01,921 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:21:01,928 - INFO - Request processed in 2.19 seconds
2025-05-13 15:21:01,928 - INFO - 127.0.0.1 - - [13/May/2025 15:21:01] "POST /chat HTTP/1.1" 200 -
2025-05-13 15:21:14,736 - INFO - 127.0.0.1 - - [13/May/2025 15:21:14] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-13 15:21:15,039 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-13 15:21:15,039 - INFO - Found API key in environment variables
2025-05-13 15:21:15,436 - INFO - [GROQ_API] API key verified successfully
2025-05-13 15:21:15,437 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-13 15:21:16,490 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-13 15:21:16,941 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-13 15:21:16,941 - INFO - Request processed in 1.90 seconds
2025-05-13 15:21:16,941 - INFO - 127.0.0.1 - - [13/May/2025 15:21:16] "POST /chat HTTP/1.1" 200 -
2025-05-14 01:12:31,345 - WARNING - Warning: faq_data.json not found. Please create it before using the FAQ endpoint.
2025-05-14 01:12:31,345 - INFO - Found API key in environment variables
2025-05-14 01:12:31,795 - INFO - [GROQ_API] API key verified successfully
2025-05-14 01:12:31,795 - INFO - Groq API key verified successfully!
2025-05-14 01:12:31,795 - INFO - Starting preload of models...
2025-05-14 01:12:31,861 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-14 01:12:31,863 - INFO - [33mPress CTRL+C to quit[0m
2025-05-14 11:37:28,087 - INFO - Found API key in environment variables
2025-05-14 11:37:28,708 - INFO - [GROQ_API] API key verified successfully
2025-05-14 11:37:28,710 - INFO - Groq API key verified successfully!
2025-05-14 11:37:28,713 - INFO - Starting preload of models...
2025-05-14 11:37:28,830 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2025-05-14 11:37:28,830 - INFO - [33mPress CTRL+C to quit[0m
2025-05-14 11:38:13,892 - INFO - 127.0.0.1 - - [14/May/2025 11:38:13] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-14 11:38:14,150 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-14 11:38:14,150 - INFO - Found API key in environment variables
2025-05-14 11:38:14,579 - INFO - [GROQ_API] API key verified successfully
2025-05-14 11:38:14,579 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-14 11:38:20,549 - INFO - Initializing embeddings model...
2025-05-14 11:38:20,549 - INFO - Initializing embeddings model...
2025-05-14 11:38:20,549 - INFO - Use pytorch device_name: cpu
2025-05-14 11:38:20,549 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-14 11:38:20,549 - INFO - Use pytorch device_name: cpu
2025-05-14 11:38:20,559 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-14 11:38:24,180 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-14 11:38:24,198 - INFO - Use pytorch device_name: cpu
2025-05-14 11:38:24,198 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-14 11:38:25,208 - ERROR - ERROR: Failed to initialize embeddings model: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
Traceback (most recent call last):
  File "C:\Users\Neha Sachdeva\OneDrive\Desktop\internship\customer_chatbot\customer_chatbot\backend\app.py", line 191, in initialize_embeddings
    EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
                       ^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'HuggingFaceEmbeddings' where it is not associated with a value
2025-05-14 11:38:25,216 - INFO - Use pytorch device_name: cpu
2025-05-14 11:38:25,216 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-05-14 11:38:26,833 - INFO - Fallback embeddings model initialized successfully
2025-05-14 11:38:26,834 - INFO - Loading existing index from faiss_en_index...
2025-05-14 11:38:26,846 - INFO - Loading faiss with AVX2 support.
2025-05-14 11:38:27,365 - INFO - Successfully loaded faiss with AVX2 support.
2025-05-14 11:38:27,380 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-14 11:38:27,413 - INFO - Successfully loaded existing vectorstore for en
2025-05-14 11:38:27,864 - INFO - Fallback embeddings model initialized successfully
2025-05-14 11:38:27,864 - INFO - Loading existing index from faiss_hi_index...
2025-05-14 11:38:27,896 - INFO - Successfully loaded existing vectorstore for hi
2025-05-14 11:38:27,896 - INFO - Model preloading completed!
2025-05-14 11:38:29,101 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-14 11:38:30,084 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-14 11:38:30,093 - INFO - Request processed in 15.95 seconds
2025-05-14 11:38:30,097 - INFO - 127.0.0.1 - - [14/May/2025 11:38:30] "POST /chat HTTP/1.1" 200 -
2025-05-14 11:42:43,926 - INFO - 127.0.0.1 - - [14/May/2025 11:42:43] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-14 11:42:44,173 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-14 11:42:44,173 - INFO - Found API key in environment variables
2025-05-14 11:42:44,712 - INFO - [GROQ_API] API key verified successfully
2025-05-14 11:42:44,712 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-14 11:42:45,996 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-14 11:42:47,657 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-14 11:42:47,659 - INFO - Request processed in 3.49 seconds
2025-05-14 11:42:47,660 - INFO - 127.0.0.1 - - [14/May/2025 11:42:47] "POST /chat HTTP/1.1" 200 -
2025-05-14 11:42:58,522 - INFO - 127.0.0.1 - - [14/May/2025 11:42:58] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-14 11:42:58,843 - INFO - 127.0.0.1 - - [14/May/2025 11:42:58] "POST /chat HTTP/1.1" 200 -
2025-05-14 11:43:10,331 - INFO - 127.0.0.1 - - [14/May/2025 11:43:10] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-14 11:43:10,596 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-14 11:43:10,597 - INFO - Found API key in environment variables
2025-05-14 11:43:11,126 - INFO - [GROQ_API] API key verified successfully
2025-05-14 11:43:11,129 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-14 11:43:12,419 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-14 11:43:13,361 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-14 11:43:13,363 - INFO - Request processed in 2.77 seconds
2025-05-14 11:43:13,365 - INFO - 127.0.0.1 - - [14/May/2025 11:43:13] "POST /chat HTTP/1.1" 200 -
2025-05-14 11:43:34,230 - INFO - 127.0.0.1 - - [14/May/2025 11:43:34] "OPTIONS /chat HTTP/1.1" 200 -
2025-05-14 11:43:34,544 - INFO - [QA_CHAIN] Creating QA Chain for language: en
2025-05-14 11:43:34,544 - INFO - Found API key in environment variables
2025-05-14 11:43:34,939 - INFO - [GROQ_API] API key verified successfully
2025-05-14 11:43:34,939 - INFO - [QA_CHAIN] API key retrieved and verified successfully
2025-05-14 11:43:36,309 - INFO - [QA_CHAIN] QA Chain created successfully
2025-05-14 11:43:37,088 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-14 11:43:37,090 - INFO - Request processed in 2.55 seconds
2025-05-14 11:43:37,092 - INFO - 127.0.0.1 - - [14/May/2025 11:43:37] "POST /chat HTTP/1.1" 200 -
